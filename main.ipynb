{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main file of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import app.var_examination as ve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background info\n",
    "\n",
    "The Boston Housing Dataset has been removed from the Scikit-Learn library in December 2022, after decades of its use for teaching purposes. Originally, the dataset comes from a research paper *Hedonic housing prices and the demand for clean air* by D. Harrison and D. Rubinfield, published in 1978. \n",
    "\n",
    "The dataset has been removed and its use is now discouraged for several reasons, the main of which is the inclusion of the `B` variable, which is a column whose values are calculated as `B=1000(Bk-063)^2` where `Bk` is, \"the proportion of blacks by town.\" The quadratic formula is motivated by an argument of the authors that race segregation has positive impact on housing prices, \n",
    "\n",
    "**The goal of this project is to examine the role of the Bk variable and show the issues with its inclussion in the dataset, as well as show the impact of the variable on standard models that were used to process this data.** \n",
    "\n",
    "We state that the inclusion of the variable in this context, and the idea of using race to predict prices without considering it as a tremendous issue, are and always will be unaccpetable.\n",
    "\n",
    "## Current state\n",
    "\n",
    "Importing the dataset in the traditional way now throws the following error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bostn = datasets.load_boston()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the data \"by hand\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation of Scikit-Learn advises users to obtain the dataset in the csv format from its original source. We respect that and then convert the data to a well-formatted Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, there are 14 variables:\n",
    "\n",
    "- **CRIM**: per capita crime rate by town\n",
    "- **ZN**: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- **INDUS**: proportion of non-retail business acres per town\n",
    "- **CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- **NOX**: nitric oxides concentration (parts per 10 million)\n",
    "- **RM**: average number of rooms per dwelling\n",
    "- **AGE**: proportion of owner-occupied units built prior to 1940\n",
    "- **DIS**: weighted distances to five Boston employment centres\n",
    "- **RAD**: index of accessibility to radial highways\n",
    "- **TAX**: full-value property-tax rate per $10,000\n",
    "- **PTRATIO**: pupil-teacher ratio by town\n",
    "- **B**: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "- **LSTAT**: % lower status of the population\n",
    "- **MEDV**: Median value of owner-occupied homes in $1000's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data as instructed by documentation\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "\n",
    "# Split the data into data and target\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "# Convert the data to a well-formatted dataframe\n",
    "\n",
    "column_names = [\n",
    "    \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\",\n",
    "    \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"\n",
    "]\n",
    "\n",
    "feature_columns = column_names[:-1]  # All columns except the target\n",
    "target_column = column_names[-1]     # The target column\n",
    "\n",
    "# Construct the features DataFrame\n",
    "boston_df = pd.DataFrame(data, columns=feature_columns)\n",
    "\n",
    "# Add the target to the DataFrame\n",
    "boston_df[target_column] = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any filled in values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if mean, median, min or max values occur suspisiously frequently in the dataset.**\n",
    "\n",
    "Values in the dataset are rounded to 6 decimal places, match that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = []\n",
    "\n",
    "# The obtained values are rounded to 6 decimal places, match that with the statistics\n",
    "\n",
    "for column in boston_df.columns:\n",
    "    mean = boston_df[column].mean().round(6)\n",
    "    median = boston_df[column].median().round(6)\n",
    "    mode = boston_df[column].mode().round(6)\n",
    "    min_value = boston_df[column].min().round(6)\n",
    "    max_value = boston_df[column].max().round(6)\n",
    "\n",
    "    mean_frequency = (boston_df[column].value_counts().get(mean, 0) / len(boston_df)) * 100\n",
    "    median_frequency = (boston_df[column].value_counts().get(median, 0) / len(boston_df)) * 100\n",
    "    mode_frequency = (boston_df[column].value_counts().get(mode[0], 0) / len(boston_df)) * 100\n",
    "    min_frequency = (boston_df[column].value_counts().get(min_value, 0) / len(boston_df)) * 100\n",
    "    max_frequency = (boston_df[column].value_counts().get(max_value, 0) / len(boston_df)) * 100\n",
    "\n",
    "    statistics.append({\n",
    "        'Column': column,\n",
    "        'Mean': f\"{mean_frequency:.2f}%\",\n",
    "        'Median': f\"{median_frequency:.2f}%\",\n",
    "        'Mode': f\"{mode_frequency:.2f}%\",\n",
    "        'Min': f\"{min_frequency:.2f}%\",\n",
    "        'Max': f\"{max_frequency:.2f}%\"\n",
    "    })\n",
    "\n",
    "statistics_df = pd.DataFrame(statistics)\n",
    "print(statistics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things arise:\n",
    "\n",
    "- **ZN** column is mostly zeroes. That makes sense as it is the proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- The high but different percentages of median and max in **RAD** are strange. \n",
    "- **B** variable is likely very skewed to the right.\n",
    "\n",
    "- The mode of 26.09% occurs far too freqeuently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = boston_df.duplicated()\n",
    "print(duplicates.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df.hist(figsize=(12, 10), bins=50, grid=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RAD** distribution is weird. Noted.\n",
    "\n",
    "**TAX** is one of the variables that have mode in 26.09% of rows. In this case, it could indicate that most households have a common tax rate, which is alright. **PTRATIO**, **INDUS**, **RAD** and **B** also have modes at or very close to 26.09%. This could indicate that there was a densly populated area that caused such significant values to occur.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LSTAT Variable\n",
    "\n",
    "One of the problematic variables is LSTAT, a percentage of \"lower status population\". The original paper describes it as \n",
    "\n",
    "> Proportion of population that is lower status = 1/2 (proportion of adults without some high education and proportion of male workers classified as laborers)\n",
    "\n",
    "And the authors suggest that \"the effect on price is higher in the upper brackets of society\", and therefore a logarithmic transform should be applied in the models.\n",
    "\n",
    "The sole inclusion and consideration of this variable does raise concerns. For now, we will simply explore its properties and pay more attention to it later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve.var_examination(boston_df, \"LSTAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The B Variable\n",
    "\n",
    "A key interest of this project is the B variable. We do not plan on commenting neither the reasons for its inclusion both in the original research paper and in the dataset, nor why the issues were recognized only a few years ago. We will try to examine the variable and later show its role in the dataset.\n",
    "\n",
    "In the original research paper, the variable is described as \n",
    "\n",
    "> Black proportion of population. At low or moderate levels of B, an increase of B should have negative on housing values if Blacks are regarded as undesirable by Whites. However, market discrimination means that market prices are higher at very high levels of B. One expects, therefore, a parabolic relationship between proportion Black in a neighborhood and housing values.\n",
    "\n",
    "We should note that the above **definition is wrong on many levels** and should not be considered at all in a real world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve.var_examination(boston_df, \"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We question the need to use the non-linear transform of `B=1000(Bk-0.63)^2`. Therefore, an attempt to extract the original value `Bk` is conducted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bk = np.linspace(0, 1, 100)\n",
    "Bf = 1000 * (Bk - 0.63) ** 2\n",
    "\n",
    "plt.plot(Bk, Bf)\n",
    "plt.xlabel('Bk')\n",
    "plt.ylabel('Bf')\n",
    "plt.title('Bf = 1000(Bk-0.63)^2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In following, we will attmpt to train linear regression model capable of predicting value of housing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston_df.drop('MEDV', axis=1)\n",
    "Y = boston_df['MEDV']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.8, random_state=42)\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n",
    "\n",
    "plt.scatter(Y_test, Y_pred)\n",
    "plt.xlabel('Actual Prices (MEDV)')\n",
    "plt.ylabel('Predicted Prices (MEDV)')\n",
    "plt.title('Actual Prices vs. Predicted Prices')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
